{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN3: Implementacja momentu i normalizacji gradientu\n",
    "Adrianna Grudzień"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_with_momentum:\n",
    "    def __init__(self, shape, activation='sigmoid'):\n",
    "        self.shape = shape\n",
    "        self.activation = activation\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self, min_val=-0.5, max_val=0.5):\n",
    "        self.weights = np.random.uniform(min_val, max_val, size=self.shape)\n",
    "        self.biases = np.random.uniform(min_val, max_val, size=self.shape[1])\n",
    "\n",
    "    def calculate(self, x):\n",
    "        return np.matmul(x, self.weights) + self.biases\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return Layer_with_momentum.sigmoid(x)\n",
    "        if self.activation == 'linear':\n",
    "            return x   \n",
    "        \n",
    "    def update(self, momentum_weights, momentum_biases, learning_rate):            \n",
    "        self.weights += np.array(np.reshape([learning_rate*i for i in momentum_weights],self.shape))\n",
    "        self.biases += np.array(np.reshape([learning_rate*i for i in momentum_biases], self.shape[1]))\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class NN_with_momentum:\n",
    "    def __init__(self, input_shape, output_size, layers_num=1, neurons_num=[5], activations=['linear']):\n",
    "        self.input_shape = input_shape\n",
    "        # self.output_size = output_size\n",
    "        self.layers_num = layers_num\n",
    "        self.neurons_num = neurons_num\n",
    "        self.activations = activations\n",
    "        self._build()\n",
    "\n",
    "    def visualise_weights(self):\n",
    "        for i in range(self.layers_num):\n",
    "            plt.subplot(self.layers_num, 1, i+1)\n",
    "            plt.scatter(np.arange(self.neurons_num[i]), self.layers[i].weights)\n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        return np.exp(-x) / np.square((1 + np.exp(-x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y_true, y_pred):\n",
    "         return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "    def calculate_errors(self, y_true, y_pred):\n",
    "        errors = []\n",
    "        for i in range(self.layers_num - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            if layer.activation == 'linear':\n",
    "                derivative = np.ones(shape=(layer.shape[-1], 1))\n",
    "            if layer.activation == 'sigmoid':\n",
    "                derivative = NN_with_momentum.sigmoid_derivative(self.recent_calculations[i + 1].reshape(layer.shape[-1], 1))\n",
    "            if i == self.layers_num - 1:\n",
    "                errors.append(np.multiply((y_pred - y_true), derivative))\n",
    "            else:\n",
    "                errors.append(np.multiply(derivative, np.dot(self.layers[i + 1].weights, errors[-1])))\n",
    "        errors.reverse()\n",
    "        return errors\n",
    "      \n",
    "    def update_layers(self):\n",
    "        for i in range(self.layers_num):  \n",
    "            self.layers[i].update(self.momentum_weights[i], self.momentum_biases[i], self.learning_rate)\n",
    "            \n",
    "    def backpropagate(self, y_pred, y_true, x):\n",
    "        delta_weights = []\n",
    "        delta_biases = []\n",
    "        \n",
    "        errors = self.calculate_errors(y_true, y_pred) \n",
    "        for i in range(self.layers_num - 1, 0, -1):     \n",
    "            a = self.layers[i - 1].activate(self.recent_calculations[i].reshape(self.layers[i - 1].shape[-1], 1))       \n",
    "            delta_weights.insert(0, -self.learning_rate * np.outer(a, errors[i]) / self.batch_size)\n",
    "            delta_biases.insert(0, -self.learning_rate * errors[i] / self.batch_size)\n",
    "        delta_weights.insert(0, -self.learning_rate * np.outer(x, errors[0]) / self.batch_size)\n",
    "        delta_biases.insert(0, -self.learning_rate * errors[0] / self.batch_size)\n",
    "        \n",
    "        return delta_weights, delta_biases\n",
    "                \n",
    "    def sum_deltas(self, delta_weights, delta_biases):\n",
    "        for i in range(self.layers_num):\n",
    "            self.delta_weights[i] += np.array(delta_weights[i])\n",
    "            self.delta_biases[i] += np.array(delta_biases[i])\n",
    "\n",
    "    def _build(self):\n",
    "        self.layers = []\n",
    "\n",
    "        layer = Layer_with_momentum(shape=(self.input_shape[1], self.neurons_num[0]), activation=self.activations[0])\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        for i in range(1, self.layers_num):\n",
    "            layer = Layer_with_momentum(shape=(self.layers[i - 1].shape[1], self.neurons_num[i]), activation=self.activations[i])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # layer = Layer_with_momentum(shape=(self.layers[-1].shape[1], self.output_size), activation=self.activations[-1])\n",
    "        # self.layers.append(layer)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_numpy_array(x_train, y_train, x_test, y_test):\n",
    "        if x_test is None or y_test is None:\n",
    "            return np.array(x_train), np.array(y_train), None, None\n",
    "        else:\n",
    "            return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "\n",
    "    def print_results(self, epoch):\n",
    "        print(f'Epoch number {epoch}/{self.n_epochs}')\n",
    "        print(f'MSE on training set: {NN_with_momentum.mse(self.y_train, self.predict(self.x_train))}', end=' ')\n",
    "        if self.x_test is not None:\n",
    "            print(f'     , MSE on test set: {NN_with_momentum.mse(self.y_test, self.predict(self.x_test))}')\n",
    "\n",
    "    def predict(self, input):\n",
    "        self.recent_calculations = []\n",
    "        self.recent_calculations.append(input)\n",
    "\n",
    "        x = self.layers[0].calculate(input)\n",
    "        self.recent_calculations.append(x)\n",
    "        x = self.layers[0].activate(x)\n",
    "        \n",
    "\n",
    "        for i in range(1, self.layers_num):\n",
    "            x = self.layers[i].calculate(x)\n",
    "            self.recent_calculations.append(x)\n",
    "            x = self.layers[i].activate(x)\n",
    "            assert ~np.isnan(x).any() \n",
    "\n",
    "        return x\n",
    "    \n",
    "    def fit(self, x_train, y_train, lambdaa, batch_size, n_epochs, learning_rate=0.003, x_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        lambdaa - współczynnik wygaszania momentu (z przedziału (0,1))\n",
    "        \"\"\"\n",
    "        self.x_train, self.y_train, self.x_test, self.y_test = NN_with_momentum.convert_to_numpy_array(x_train, y_train, x_test, y_test)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        n = y_train.shape[0]\n",
    "        self.momentum_weights = np.zeros((1,n))\n",
    "        self.momentum_biases = np.zeros((1,n))\n",
    "\n",
    "        indices = np.arange(n)\n",
    "        epoch = 1\n",
    "        while(epoch <= self.n_epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            mini_batches = np.split(indices, [i * batch_size for i in range(1, n // batch_size)])\n",
    "            for batch in mini_batches:      \n",
    "                for j in range(batch_size):\n",
    "                    y_pred = self.predict(self.x_train[batch[j]])\n",
    "                    if j == 0:\n",
    "                        self.delta_weights, self.delta_biases = self.backpropagate(y_pred, self.y_train[batch[j]], self.x_train[batch[j]]) # inicjalizowanie delta_weights i delta_bias\n",
    "                    else:   \n",
    "                        temp_delta_weights, temp_delta_biases = self.backpropagate(y_pred, self.y_train[batch[j]], self.x_train[batch[j]])\n",
    "                        self.sum_deltas(temp_delta_weights, temp_delta_biases)\n",
    "                  \n",
    "\n",
    "                self.momentum_weights = self.delta_weights + [i*lambdaa for i in self.momentum_weights]\n",
    "                self.momentum_biases = self.delta_biases + [i*lambdaa for i in self.momentum_biases]\n",
    "        \n",
    "                self.update_layers()\n",
    "\n",
    "            if epoch % 30 == 0:\n",
    "                 self.print_results(epoch)\n",
    "            epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = pd.read_csv('mio1/regression/square-simple-training.csv', index_col=0)\n",
    "sq_x_train = np.reshape(np.array(sq.x), (100, 1))\n",
    "sq_y_train = np.reshape(np.array(sq.y), (100, 1))\n",
    "\n",
    "\n",
    "sq_test = pd.read_csv('mio1/regression/square-simple-test.csv', index_col=0)\n",
    "sq_x_test = np.reshape(np.array(sq_test.x), (100, 1))\n",
    "sq_y_test = np.reshape(np.array(sq_test.y), (100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 30/700\n",
      "MSE on training set: 10107.44357736024      , MSE on test set: 8695.858528843639\n",
      "Epoch number 60/700\n",
      "MSE on training set: 10041.77448139226      , MSE on test set: 8614.297748578883\n",
      "Epoch number 90/700\n",
      "MSE on training set: 9980.423372846317      , MSE on test set: 8537.610428564465\n",
      "Epoch number 120/700\n",
      "MSE on training set: 9923.032732534652      , MSE on test set: 8465.40926802242\n",
      "Epoch number 150/700\n",
      "MSE on training set: 9869.279260668754      , MSE on test set: 8397.344467929926\n",
      "Epoch number 180/700\n",
      "MSE on training set: 9818.868043914683      , MSE on test set: 8333.097040929351\n",
      "Epoch number 210/700\n",
      "MSE on training set: 9771.525194178746      , MSE on test set: 8272.36982046207\n",
      "Epoch number 240/700\n",
      "MSE on training set: 9727.008399655013      , MSE on test set: 8214.901729486266\n",
      "Epoch number 270/700\n",
      "MSE on training set: 9685.086618613595      , MSE on test set: 8160.441766562543\n",
      "Epoch number 300/700\n",
      "MSE on training set: 9645.552637920619      , MSE on test set: 8108.765895221573\n",
      "Epoch number 330/700\n",
      "MSE on training set: 9608.214509181693      , MSE on test set: 8059.666139080132\n",
      "Epoch number 360/700\n",
      "MSE on training set: 9572.894036713928      , MSE on test set: 8012.94886523541\n",
      "Epoch number 390/700\n",
      "MSE on training set: 9539.429638652033      , MSE on test set: 7968.439069385217\n",
      "Epoch number 420/700\n",
      "MSE on training set: 9507.669291465834      , MSE on test set: 7925.971030791007\n",
      "Epoch number 450/700\n",
      "MSE on training set: 9477.473520112888      , MSE on test set: 7885.392558753232\n",
      "Epoch number 480/700\n",
      "MSE on training set: 9448.71759525532      , MSE on test set: 7846.568892349605\n",
      "Epoch number 510/700\n",
      "MSE on training set: 9421.281080693625      , MSE on test set: 7809.367726399298\n",
      "Epoch number 540/700\n",
      "MSE on training set: 9395.055373735411      , MSE on test set: 7773.670518526666\n",
      "Epoch number 570/700\n",
      "MSE on training set: 9369.937428974621      , MSE on test set: 7739.363367576696\n",
      "Epoch number 600/700\n",
      "MSE on training set: 9345.835336561182      , MSE on test set: 7706.345734594169\n",
      "Epoch number 630/700\n",
      "MSE on training set: 9322.661843262154      , MSE on test set: 7674.520864626531\n",
      "Epoch number 660/700\n",
      "MSE on training set: 9300.337848590241      , MSE on test set: 7643.801429368219\n",
      "Epoch number 690/700\n",
      "MSE on training set: 9278.788270822513      , MSE on test set: 7614.103205982322\n"
     ]
    }
   ],
   "source": [
    "nn_with_momentum_sq = NN_with_momentum(input_shape=[len(sq_x_train), 1], output_size=1, layers_num=2, neurons_num=[90, 1], activations=['sigmoid', 'linear'])\n",
    "nn_with_momentum_sq.fit(x_train=sq_x_train, y_train=sq_y_train, lambdaa=0.8, batch_size=2, n_epochs=700, x_test=sq_x_test, y_test=sq_y_test, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sq_x_test, sq_y_test, color='green')\n",
    "plt.scatter(sq_x_test, nn_with_momentum_sq.predict(sq_x_test), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbiór steps-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_train = pd.read_csv('mio1/regression/steps-small-training.csv', index_col=0)\n",
    "ss_x_train = np.reshape(np.array(ss_train.x), (len(ss_train.x), 1))\n",
    "ss_y_train = np.reshape(np.array(ss_train.y), (len(ss_train.x), 1))\n",
    "\n",
    "\n",
    "ss_test = pd.read_csv('mio1/regression/steps-small-test.csv', index_col=0)\n",
    "ss_x_test = np.reshape(np.array(ss_test.x), (len(ss_test.x), 1))\n",
    "ss_y_test = np.reshape(np.array(ss_test.y), (len(ss_test.x), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_with_momentum_ss = NN_with_momentum(input_shape=[len(ss_x_train), 1], output_size=1, layers_num=2, neurons_num=[100, 1], activations=['sigmoid', 'linear'])\n",
    "nn_with_momentum_ss.fit(x_train=ss_x_train, y_train=ss_y_train, lambdaa=0.8, batch_size=2, n_epochs=700, x_test=ss_x_test, y_test=ss_y_test, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ss_x_test, ss_y_test, color='green')\n",
    "plt.scatter(ss_x_test, nn_with_momentum_ss.predict(ss_x_test), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbiór multimodal-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = pd.read_csv('mio1/regression/multimodal-large-training.csv')\n",
    "ml_x_train = np.reshape(np.array(ml.x), (len(ml.x), 1))\n",
    "ml_y_train = np.reshape(np.array(ml.y), (len(ml.x), 1))\n",
    "\n",
    "\n",
    "ml_test = pd.read_csv('mio1/regression/multimodal-large-test.csv')\n",
    "ml_x_test = np.reshape(np.array(ml_test.x), (len(ml_test.x), 1))\n",
    "ml_y_test = np.reshape(np.array(ml_test.y), (len(ml_test.x), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_with_momentum_ml = NN_with_momentum(input_shape=[len(ml_x_train), 1], output_size=1, layers_num=2, neurons_num=[100, 1], activations=['sigmoid', 'linear'])\n",
    "nn_with_momentum_ml.fit(x_train=ml_x_train, y_train=ml_y_train, lambdaa=0.8, batch_size=2, n_epochs=700, x_test=ml_x_test, y_test=ml_y_test, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ml_x_test, ml_y_test, color='green')\n",
    "plt.scatter(ml_x_test, nn_with_momentum_ss.predict(ml_x_test), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nn_sq.fit(x_train=sq_x_train, y_train=sq_y_train, batch_size=2, n_epochs=2000, x_test=sq_x_test, y_test=sq_y_test, learning_rate=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nn_with_momentum_sq.fit(x_train=sq_x_train, y_train=sq_y_train, lambdaa=0.7, batch_size=2, n_epochs=700, x_test=sq_x_test, y_test=sq_y_test, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_with_RMSProp:\n",
    "    def __init__(self, shape, activation='sigmoid'):\n",
    "        self.shape = shape\n",
    "        self.activation = activation\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self, min_val=-0.5, max_val=0.5):\n",
    "        self.weights = np.random.uniform(min_val, max_val, size=self.shape)\n",
    "        self.biases = np.random.uniform(min_val, max_val, size=self.shape[1])\n",
    "\n",
    "    def calculate(self, x):\n",
    "        return np.matmul(x, self.weights) + self.biases\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return Layer_with_RMSProp.sigmoid(x)\n",
    "        if self.activation == 'linear':\n",
    "            return x   \n",
    "        \n",
    "    def update(self, g_weights, g_biases, e_g2_weights, e_g2_biases, learning_rate):\n",
    "#         self.weights -= np.array(np.reshape([learning_rate*g_weights[i]/\n",
    "#                                              np.sqrt(e_g2_weights[i]) for i in len(e_g2_weights)],self.shape))\n",
    "#         self.biases -= np.array(np.reshape([learning_rate*g_biases[i]/np.sqrt(e_g2_biases[i]) for i in len(e_g2_biases)],self.shape))\n",
    "        for j in range(len(e_g2_weights)):\n",
    "            print(g_weights[j])\n",
    "            print(e_g2_weights[j])\n",
    "            print(self.weights[j])\n",
    "            \n",
    "            self.weights[j] -= learning_rate*np.asmatrix(g_weights[j])\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class NN_with_RMSProp:\n",
    "    def __init__(self, input_shape, output_size, layers_num=1, neurons_num=[5], activations=['linear']):\n",
    "        self.input_shape = input_shape\n",
    "        # self.output_size = output_size\n",
    "        self.layers_num = layers_num\n",
    "        self.neurons_num = neurons_num\n",
    "        self.activations = activations\n",
    "        self._build()\n",
    "\n",
    "    def visualise_weights(self):\n",
    "        for i in range(self.layers_num):\n",
    "            plt.subplot(self.layers_num, 1, i+1)\n",
    "            plt.scatter(np.arange(self.neurons_num[i]), self.layers[i].weights)\n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        return np.exp(-x) / np.square((1 + np.exp(-x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y_true, y_pred):\n",
    "         return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "    def calculate_errors(self, y_true, y_pred):\n",
    "        errors = []\n",
    "        for i in range(self.layers_num - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            if layer.activation == 'linear':\n",
    "                derivative = np.ones(shape=(layer.shape[-1], 1))\n",
    "            if layer.activation == 'sigmoid':\n",
    "                derivative = NN_with_RMSProp.sigmoid_derivative(self.recent_calculations[i + 1].reshape(layer.shape[-1], 1))\n",
    "            if i == self.layers_num - 1:\n",
    "                errors.append(np.multiply((y_pred - y_true), derivative))\n",
    "            else:\n",
    "                errors.append(np.multiply(derivative, np.dot(self.layers[i + 1].weights, errors[-1])))\n",
    "        errors.reverse()\n",
    "        return errors\n",
    "      \n",
    "    def update_layers(self):\n",
    "        # print(self.delta_weights)\n",
    "        for i in range(self.layers_num):  \n",
    "            self.layers[i].update(self.g_weights[i], self.g_biases[i], self.e_g2_weights[i], self.e_g2_biases[i], self.learning_rate)\n",
    "            \n",
    "    def backpropagate(self, y_pred, y_true, x):\n",
    "        delta_weights = []\n",
    "        delta_biases = []\n",
    "        \n",
    "        errors = self.calculate_errors(y_true, y_pred) \n",
    "        for i in range(self.layers_num - 1, 0, -1):     \n",
    "            a = self.layers[i - 1].activate(self.recent_calculations[i].reshape(self.layers[i - 1].shape[-1], 1))       \n",
    "            delta_weights.insert(0, self.learning_rate * np.outer(a, errors[i]) / self.batch_size)\n",
    "            # print(f'Deltas: {delta_weights[0]}')\n",
    "            delta_biases.insert(0, self.learning_rate * errors[i] / self.batch_size)\n",
    "        delta_weights.insert(0, self.learning_rate * np.outer(x, errors[0]) / self.batch_size)\n",
    "        # print(f'Deltas: {delta_weights[0]}')\n",
    "        delta_biases.insert(0, self.learning_rate * errors[0] / self.batch_size)\n",
    "        \n",
    "        return delta_weights, delta_biases\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        return delta_weights, delta_biases\n",
    "        \n",
    "    def sum_deltas(self, g_weights, g_biases):\n",
    "        for i in range(self.layers_num):\n",
    "            self.g_weights[i] += np.array(g_weights[i])\n",
    "            self.g_biases[i] += np.array(g_biases[i])\n",
    "\n",
    "    def _build(self):\n",
    "        self.layers = []\n",
    "\n",
    "        layer = Layer_with_RMSProp(shape=(self.input_shape[1], self.neurons_num[0]), activation=self.activations[0])\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        for i in range(1, self.layers_num):\n",
    "            layer = Layer_with_RMSProp(shape=(self.layers[i - 1].shape[1], self.neurons_num[i]), activation=self.activations[i])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # layer = Layer_with_momentum(shape=(self.layers[-1].shape[1], self.output_size), activation=self.activations[-1])\n",
    "        # self.layers.append(layer)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_numpy_array(x_train, y_train, x_test, y_test):\n",
    "        if x_test is None or y_test is None:\n",
    "            return np.array(x_train), np.array(y_train), None, None\n",
    "        else:\n",
    "            return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "\n",
    "    def print_results(self, epoch):\n",
    "        print(f'Epoch number {epoch}/{self.n_epochs}')\n",
    "        print(f'MSE on training set: {NN_with_RMSProp.mse(self.y_train, self.predict(self.x_train))}', end=' ')\n",
    "        if self.x_test is not None:\n",
    "            print(f'     , MSE on test set: {NN_with_RMSProp.mse(self.y_test, self.predict(self.x_test))}')\n",
    "\n",
    "    def predict(self, input):\n",
    "        self.recent_calculations = []\n",
    "        self.recent_calculations.append(input)\n",
    "\n",
    "        x = self.layers[0].calculate(input)\n",
    "        self.recent_calculations.append(x)\n",
    "        x = self.layers[0].activate(x)\n",
    "        \n",
    "\n",
    "        for i in range(1, self.layers_num):\n",
    "            x = self.layers[i].calculate(x)\n",
    "            self.recent_calculations.append(x)\n",
    "            x = self.layers[i].activate(x)\n",
    "            assert ~np.isnan(x).any() \n",
    "\n",
    "        return x\n",
    "    \n",
    "    def fit(self, x_train, y_train, beta, batch_size, n_epochs, learning_rate=0.003, x_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        beta - współczynnik wygaszania\n",
    "        \"\"\"\n",
    "        self.x_train, self.y_train, self.x_test, self.y_test = NN_with_RMSProp.convert_to_numpy_array(x_train, y_train, x_test, y_test)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        n = y_train.shape[0]\n",
    "        self.e_g2_weights = np.zeros((1,n)) # E[g^2]\n",
    "        self.e_g2_biases = np.zeros((1,n)) # E[g^2]\n",
    "\n",
    "        indices = np.arange(n)\n",
    "        epoch = 1\n",
    "        while(epoch <= self.n_epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            mini_batches = np.split(indices, [i * batch_size for i in range(1, n // batch_size)])\n",
    "            for batch in mini_batches:      \n",
    "                for j in range(batch_size):\n",
    "                    y_pred = self.predict(self.x_train[batch[j]])\n",
    "                    if j == 0:\n",
    "                        self.g_weights, self.g_biases = self.backpropagate(y_pred, self.y_train[batch[j]], self.x_train[batch[j]]) # inicjalizowanie g_weights i g_bias\n",
    "                    else:   \n",
    "                        temp_g_weights, temp_g_biases = self.backpropagate(y_pred, self.y_train[batch[j]], self.x_train[batch[j]])\n",
    "                        self.sum_deltas(temp_g_weights, temp_g_biases)\n",
    "\n",
    "#                 self.momentum_weights = self.delta_weights + [i*lambdaa for i in self.momentum_weights]\n",
    "#                 self.momentum_biases = self.delta_biases + [i*lambdaa for i in self.momentum_biases]\n",
    "        \n",
    "                print(self.e_g2_weights.shape)\n",
    "                self.e_g2_weights = np.array([beta*i for i in self.e_g2_weights] + [(1-beta)*i**2 for i in self.g_weights])\n",
    "                self.e_g2_biases = np.array([beta*i for i in self.e_g2_biases] + [(1-beta)*i**2 for i in self.g_biases])\n",
    "\n",
    "                self.update_layers()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                 self.print_results(epoch)\n",
    "            epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "[-2.79718399e-03  1.13068074e-03  9.24006534e-04  2.90639781e-03\n",
      "  6.21524081e-04  2.38092833e-03 -2.64894228e-03 -7.33249527e-04\n",
      " -2.30186553e-04  5.29864763e-04  1.33507079e-03 -5.75942616e-04\n",
      " -3.13293228e-03 -1.99170208e-03 -2.60668003e-03 -2.14367639e-04\n",
      "  2.37337607e-03 -3.17795500e-04 -2.37436677e-03  1.45228892e-03\n",
      "  6.33825980e-04 -2.97986062e-03 -2.45974909e-03  2.73374694e-04\n",
      " -1.55961466e-03 -1.40639901e-04  1.94787061e-03  2.51543471e-03\n",
      "  2.62741897e-03  1.30806812e-04  2.27806144e-03 -2.32299657e-03\n",
      " -8.16273995e-04 -1.37928033e-03 -2.19746398e-03 -8.75721661e-04\n",
      " -2.87346001e-04 -1.80544709e-03  2.35048770e-03 -1.24232241e-04\n",
      " -2.30705085e-03 -1.77479880e-03 -7.79298925e-04  1.04125468e-03\n",
      " -2.23964092e-03 -1.44271184e-03  3.03563533e-03  2.88042653e-03\n",
      " -2.78477991e-03 -2.21431402e-03 -2.17750320e-03 -1.83036635e-03\n",
      "  1.89597436e-03 -2.50428645e-03  2.00459547e-03  3.03756720e-04\n",
      " -2.30776065e-03 -1.25137790e-03 -1.48633240e-04  2.05896065e-03\n",
      "  1.96715453e-04  2.51904543e-03 -2.64723582e-03  6.91551408e-05\n",
      "  2.80767921e-03  1.48306258e-03  5.55027757e-04  1.03448831e-03\n",
      "  2.50961021e-04 -1.21921927e-03  2.81763574e-03 -3.27658835e-04\n",
      "  2.09394727e-04  7.15893861e-04  7.67855595e-04 -2.01554286e-03\n",
      "  2.29309821e-03 -8.77793509e-04 -1.58007405e-03  3.71764352e-04\n",
      " -2.58705649e-03 -1.57469591e-03  6.44879247e-04 -1.70790590e-03\n",
      " -1.64392088e-03  1.25771728e-03  1.82713590e-03 -7.78357917e-04\n",
      " -7.95082705e-04 -2.81794352e-03  1.09945734e-03  2.72976320e-03\n",
      " -2.39348496e-03  2.35469951e-03  3.09785706e-03  3.15950569e-03\n",
      " -5.38246188e-04 -1.34701697e-03 -2.66489563e-03 -1.48637307e-03]\n",
      "0.0\n",
      "[-0.49855122 -0.09178663  0.22964208  0.46845525 -0.00661552 -0.4457237\n",
      "  0.10241229  0.19262146 -0.1449857  -0.22350446  0.0444447   0.3884863\n",
      "  0.09448303  0.1766417  -0.17017854  0.25397558  0.12202631  0.04993924\n",
      " -0.10963853  0.16387873  0.47158541 -0.3979389  -0.37631151  0.42392767\n",
      " -0.11498223  0.25163407  0.32341894 -0.27102419 -0.14695537 -0.33056991\n",
      "  0.478189    0.33569314  0.18944075 -0.26301101 -0.19265161  0.34153456\n",
      "  0.2615269   0.21263486  0.33705231  0.05671283 -0.12711994 -0.33084339\n",
      " -0.49152086 -0.01797495  0.24289519  0.3958257  -0.12876333 -0.02809674\n",
      "  0.36649974  0.18441948  0.06766334  0.15989948  0.32765948 -0.39691977\n",
      " -0.13765963  0.18739295 -0.01702464 -0.13784406 -0.03790533 -0.16265066\n",
      " -0.0633946   0.44769805 -0.36950838 -0.19173757 -0.38082166 -0.0799797\n",
      "  0.09233862  0.41845172 -0.06999529  0.18906532 -0.40740723  0.1506773\n",
      "  0.11128236  0.47654437  0.34559969  0.03294155  0.47264523 -0.25933417\n",
      " -0.38887152  0.10354942  0.31407716  0.46457299 -0.25727651  0.18915316\n",
      " -0.02519098 -0.02954814  0.25548981 -0.31036913  0.17477272 -0.15715886\n",
      "  0.41146117  0.04340584  0.35058449  0.04499558  0.48888204 -0.36929435\n",
      "  0.30977585 -0.20541853 -0.35794727  0.14417522]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-f5526b8c6328>:179: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.e_g2_weights = np.array([beta*i for i in self.e_g2_weights] + [(1-beta)*i**2 for i in self.g_weights])\n",
      "<ipython-input-107-f5526b8c6328>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.e_g2_biases = np.array([beta*i for i in self.e_g2_biases] + [(1-beta)*i**2 for i in self.g_biases])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (100,) doesn't match the broadcast shape (1,100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-7ff4fd51c284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn_with_rmsprop_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_with_RMSProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msq_x_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'linear'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn_with_rmsprop_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msq_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msq_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msq_x_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msq_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-107-f5526b8c6328>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, beta, batch_size, n_epochs, learning_rate, x_test, y_test)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;31m#                 print(self.e_g2_weights[2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-f5526b8c6328>\u001b[0m in \u001b[0;36mupdate_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# print(self.delta_weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_biases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_g2_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_g2_biases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-f5526b8c6328>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, g_weights, g_biases, e_g2_weights, e_g2_biases, learning_rate)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (100,) doesn't match the broadcast shape (1,100)"
     ]
    }
   ],
   "source": [
    "nn_with_rmsprop_sq = NN_with_RMSProp(input_shape=[len(sq_x_train), 1], output_size=1, layers_num=2, neurons_num=[100, 1], activations=['sigmoid', 'linear'])\n",
    "nn_with_rmsprop_sq.fit(x_train=sq_x_train, y_train=sq_y_train, beta=0.7, batch_size=2, n_epochs=700, x_test=sq_x_test, y_test=sq_y_test, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*3**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
